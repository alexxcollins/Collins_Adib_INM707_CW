{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqQFG-UPmaxF"
   },
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Q-UhB7NQb3hO"
   },
   "outputs": [],
   "source": [
    "from RobotEnvClass import Q_Learning, SARSA_learning, Q_Learning_Randomness\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRbe5y0ymlZ0"
   },
   "source": [
    "# **Write some functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Yhd_mXEGb4qT"
   },
   "outputs": [],
   "source": [
    "def find_path(Q):\n",
    "    state = 6\n",
    "    path = [state]\n",
    "    #path = []\n",
    "    end_state = False\n",
    "    \n",
    "    while not end_state:\n",
    "        old_state = state\n",
    "        state = np.where(Q[old_state,] == Q[old_state,].max())[0][0]\n",
    "        if state not in path:\n",
    "            path.append(state)\n",
    "            if state == 35:\n",
    "                end_state = True\n",
    "                \n",
    "        elif state == old_state:\n",
    "            print(\"The Agent chose to stay in his position\")\n",
    "            end_state = True\n",
    "            \n",
    "        else:\n",
    "            print(\"The Agent stucked into a loop\")\n",
    "            end_state = True\n",
    "    \n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "P6GVDTNTb52j"
   },
   "outputs": [],
   "source": [
    "def calculate_rewards(R_matrix, path):\n",
    "    r = 0\n",
    "    steps = 0\n",
    "    for idx in range(len(path)-1):\n",
    "        r += R_matrix[path[idx], path[idx+1]]\n",
    "        steps += 1\n",
    "    return r,steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiUMRqbSmuwa"
   },
   "source": [
    "# **Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Dhkyyw6m1j6"
   },
   "source": [
    "## **Experiments 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment:\n",
    "* Time Rewards: 0\n",
    "* pond Rewards: 0\n",
    "* croissant Rewards: 0\n",
    "* cogs Rewards: 0\n",
    "* word Rewards: 20\n",
    "\n",
    "Alpha = 0.1\n",
    "Gamma = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NuiOxXVWb7eV"
   },
   "outputs": [],
   "source": [
    "q_learning = Q_Learning()\n",
    "q_learning.max_episodes = 1000\n",
    "q_learning.rewards={'r_time':0,'r_pond':0,'r_croissant':0,'r_cogs':0,'r_work':20}\n",
    "q_learning.tubes=[[(0, 0), (1,5)], [(1, 2), (4, 1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndnvuxylb8t8",
    "outputId": "7aca137b-dd69-4876-9c57-b2f68b61b2ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.81 s, sys: 191 ms, total: 5 s\n",
      "Wall time: 4.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.6,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6Ilh8RXcDCw",
    "outputId": "656d05b7-7ac2-4679-a528-e1a215cfc061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  [6, 0, 11, 10, 16, 17, 23, 29, 35]\n",
      "Total Rewards:  20.0\n",
      "Number of steps:  8\n"
     ]
    }
   ],
   "source": [
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy Peasy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTuF0IiynCa5"
   },
   "source": [
    "## **Experiments 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment:\n",
    "* Time Rewards: -1\n",
    "* pond Rewards: 0\n",
    "* croissant Rewards: 0\n",
    "* cogs Rewards: 0\n",
    "* word Rewards: 20\n",
    "\n",
    "Alpha = 0.1\n",
    "Gamma = 0.8\n",
    "\n",
    "We have to change the hyperparams to make the agent learn and avoid loops.\n",
    "If we work with gamma less than 0.8 the agent won't learn and he will stuck in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "np9Ya3BbcLLR"
   },
   "outputs": [],
   "source": [
    "q_learning = Q_Learning()\n",
    "q_learning.max_episodes = 1000\n",
    "q_learning.rewards={'r_time':-1,'r_pond':0,'r_croissant':0,'r_cogs':0,'r_work':20}\n",
    "q_learning.tubes=[[(0, 0), (1, 5)], [(1, 2), (5, 2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afbC9FlRcMoc",
    "outputId": "b302ae21-177e-4d55-c4f0-5984625a1781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.68 s, sys: 171 ms, total: 4.85 s\n",
      "Wall time: 4.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.8,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIjCCcc4cOCT",
    "outputId": "24ebf161-0dae-4051-8575-f75880cb101f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  [6, 0, 11, 10, 16, 17, 23, 29, 35]\n",
      "Total Rewards:  14.0\n",
      "Number of steps:  8\n"
     ]
    }
   ],
   "source": [
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Inq0rP2qnJXI"
   },
   "source": [
    "## **Experiments 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment:\n",
    "* Time Rewards: -5\n",
    "* pond Rewards: 0\n",
    "* croissant Rewards: 0\n",
    "* cogs Rewards: 0\n",
    "* word Rewards: 20\n",
    "\n",
    "Alpha = 0.1\n",
    "Gamma = 0.8\n",
    "\n",
    "Let's try if the agent can learn to stay in his position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XrYcRQTWki-C"
   },
   "outputs": [],
   "source": [
    "q_learning = Q_Learning()\n",
    "q_learning.max_episodes = 100\n",
    "q_learning.rewards={'r_time':-5,'r_pond':0,'r_croissant':0,'r_cogs':0,'r_work':20}\n",
    "q_learning.tubes=[[(0, 0), (1, 5)], [(1, 2), (4, 1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMo2GfhCkkh0",
    "outputId": "3dcc29f2-84e5-498d-e49c-4406c4745f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 778 ms, sys: 35.1 ms, total: 813 ms\n",
      "Wall time: 769 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.8,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GtyKiYS7kmjX",
    "outputId": "e7cd8bc9-0ad1-4463-db00-de6b47d09bfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Agent chose to stay in his position\n",
      "Path:  [6]\n",
      "Total Rewards:  0\n",
      "Number of steps:  0\n"
     ]
    }
   ],
   "source": [
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_opfVn9nMj-"
   },
   "source": [
    "## **Experiments 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment:\n",
    "* Time Rewards: -0.5\n",
    "* pond Rewards: -5\n",
    "* croissant Rewards: 0\n",
    "* cogs Rewards: 0\n",
    "* word Rewards: 20\n",
    "\n",
    "Alpha = 0.1\n",
    "Gamma = 0.9\n",
    "\n",
    "In Experiment 2, the agent found the best path to take to acheive his destination.\n",
    "\n",
    "* Path = [6, 0, 11, 10, 16, 22, 23, 29, 35]\n",
    "* Rewards= 14.0\n",
    "\n",
    "Now we will change the tube to be from [0,0] to [0,5]\n",
    "\n",
    "On his way, his passed throught 16. Now we assigned -5 if the agent falls into a pond.\n",
    "Let's see what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "YtKV4vWgnPMS"
   },
   "outputs": [],
   "source": [
    "q_learning = Q_Learning()\n",
    "q_learning.max_episodes = 3000\n",
    "q_learning.rewards={'r_time':-0.5,'r_pond':-5,'r_croissant':0,'r_cogs':0,'r_work':20}\n",
    "q_learning.tubes=[[(0, 0), (1, 5)], [(1, 2), (5, 2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEPw7HvxnTYo",
    "outputId": "534c27ec-408b-4158-82d0-c808d7485dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 s, sys: 363 ms, total: 15.1 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.9,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KagjD28onUiC",
    "outputId": "69352cff-5b7c-4ebc-a1f0-84f131e071f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  [6, 7, 8, 14, 20, 21, 22, 23, 29, 35]\n",
      "Total Rewards:  16.0\n",
      "Number of steps:  9\n"
     ]
    }
   ],
   "source": [
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent chose not to use the tube! \n",
    "How clever you are my agent!\n",
    "\n",
    "The path using the tube will takes 10 steps, but the agent found a path with 9 steps only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXA1NNb1nYcl"
   },
   "source": [
    "## **Experiment 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment:\n",
    "* Time Rewards: -0.5\n",
    "* pond Rewards: -5\n",
    "* croissant Rewards: 0\n",
    "* cogs Rewards: 0\n",
    "* word Rewards: 10\n",
    "\n",
    "Alpha = 0.1\n",
    "Gamma = 0.9\n",
    "\n",
    "Now we will change the blue tube to be from [1,2] to [5,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "syOCOGoinYDo"
   },
   "outputs": [],
   "source": [
    "q_learning = Q_Learning()\n",
    "q_learning.max_episodes = 1000\n",
    "q_learning.rewards={'r_time':-0.5,'r_pond':-5,'r_croissant':0,'r_cogs':0,'r_work':10}\n",
    "q_learning.tubes=[[(0, 0), (1, 5)], [(1, 2), (5, 3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EuMvFoscngOw",
    "outputId": "9f3c153e-2e4a-4402-eb30-f6867238855b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.09 s, sys: 175 ms, total: 4.26 s\n",
      "Wall time: 3.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.9,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JH19AAl9niTj",
    "outputId": "f099e649-ab10-4558-f8da-98d26fc6007a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  [6, 7, 8, 33, 34, 35]\n",
      "Total Rewards:  8.0\n",
      "Number of steps:  5\n"
     ]
    }
   ],
   "source": [
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agend learned how to use the bleu tube now, and acheive his destination with only 5 moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwUL7hJCnnbJ"
   },
   "source": [
    "## **Experiment 6:** Limitation of Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what if we have positive rewards other than going to work. what the agent reaction will be ?\n",
    "\n",
    "In this experiment:\n",
    "* Time Rewards: 0\n",
    "* pond Rewards: -1\n",
    "* croissant Rewards: 5\n",
    "* cogs Rewards: 10\n",
    "* word Rewards: 20\n",
    "\n",
    "Alpha = 0.1\n",
    "Gamma = 0.9\n",
    "Epsilon = 0.2\n",
    "\n",
    "Now we will change the red tube to be from [0,0] to [3,4], and the blue tube to be from [1,2] to [5,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ufWU6ex1nmwV"
   },
   "outputs": [],
   "source": [
    "q_learning = Q_Learning()\n",
    "q_learning.max_episodes = 1000\n",
    "q_learning.rewards={'r_time':0,'r_pond':-1,'r_croissant':5,'r_cogs':10,'r_work':20}\n",
    "q_learning.tubes=[[(0, 0), (1,5)], [(1, 2), (5, 1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UthadUDzntFw",
    "outputId": "3b330b33-7880-4e2d-8d1b-4bd2b3fd18fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 400 ms, total: 12.3 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wiHRT0l6nvJ_",
    "outputId": "2c2b75f5-a581-4b11-9603-11d55dd6aed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  [6, 0, 11, 10, 9, 15, 21, 22, 28, 34, 35]\n",
      "Total Rewards:  20.0\n",
      "Number of steps:  10\n"
     ]
    }
   ],
   "source": [
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to run it again maybe we won't get the same resutl, because the epsilon is very small.\n",
    "\n",
    "We are using small value of epsilon because we don't want our agent to explore to much and get stucked in a loop between cells 32 and 26. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQJB_vXNnz7q",
    "outputId": "aba56abd-4d8e-4a73-e7ac-fe01783f5488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  [6, 7, 13, 19, 20, 26, 27, 33, 34, 35]\n",
      "Total Rewards:  19.0\n",
      "Number of steps:  9\n",
      "CPU times: user 5.82 s, sys: 201 ms, total: 6.02 s\n",
      "Wall time: 5.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.8,0.2)\n",
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Experiments",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
