{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "490ef1d9-d906-4671-b250-2fc44d42f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RobotEnvClass import Q_Learning, SARSA_learning, Q_Learning_Randomness\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e879796-93cf-4156-a2fa-ae896cb75f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_path(Q):\n",
    "    state = 6\n",
    "    path = [state]\n",
    "    #path = []\n",
    "    end_state = False\n",
    "    \n",
    "    while not end_state:\n",
    "        old_state = state\n",
    "        state = np.where(Q[old_state,] == Q[old_state,].max())[0][0]\n",
    "        if state not in path:\n",
    "            path.append(state)\n",
    "            if state == 35:\n",
    "                end_state = True\n",
    "                \n",
    "        elif state == old_state:\n",
    "            print(\"The Agent chose to stay in his position\")\n",
    "            end_state = True\n",
    "            \n",
    "        else:\n",
    "            print(\"The Agent stucked into a loop\")\n",
    "            end_state = True\n",
    "    \n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeaf9adc-4347-4c80-ab01-9d4fa4f3e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rewards(R_matrix, path):\n",
    "    r = 0\n",
    "    steps = 0\n",
    "    for idx in range(len(path)-1):\n",
    "        r += R_matrix[path[idx], path[idx+1]]\n",
    "        steps += 1\n",
    "    return r,steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54182045-20e2-4512-98d8-ccee87ce707f",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526fd5d-9204-4698-b01f-006d1bb8ef98",
   "metadata": {},
   "source": [
    "In this experiment:\n",
    "* Time Rewards: 0\n",
    "* pond Rewards: 0\n",
    "* croissant Rewards: 0\n",
    "* cogs Rewards: 0\n",
    "* word Rewards: 20\n",
    "\n",
    "Alpha = 0.1\n",
    "Gamma = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080c141e-eeb6-4db5-9095-4861fcec0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning = Q_Learning()\n",
    "q_learning.max_episodes = 100\n",
    "q_learning.rewards={'r_time':0,'r_pond':0,'r_croissant':0,'r_cogs':0,'r_work':20}\n",
    "q_learning.tubes=[[(0, 0), (1,5)], [(1, 2), (4, 1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ffd1072-d278-4d2b-bda9-1cabeacc4c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[4, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.1\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[10, 10] = 0.0\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[16, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[9, 10] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[11, 10] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.1\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[32, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "R[26, 32] = 0.0\n",
      "0.0\n",
      "R[33, 32] = 0.0\n",
      "0.0\n",
      "R[31, 32] = 0.0\n",
      "0.0\n",
      "CPU times: user 762 ms, sys: 100 ms, total: 862 ms\n",
      "Wall time: 833 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.6,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b2f903-3c45-4bbb-a4e7-a04f8c24a3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  [6, 7, 8, 25, 26, 27, 28, 29, 35]\n",
      "Total Rewards:  20.0\n",
      "Number of steps:  8\n"
     ]
    }
   ],
   "source": [
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44276329-b76d-475c-a243-3bbadaecb32f",
   "metadata": {},
   "source": [
    "Easy Peasy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e5edcb-a5ed-4625-8c66-8aafa33fee13",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921396b-7e73-43d6-bdd6-4c4fb494960d",
   "metadata": {},
   "source": [
    "In this experiment:\n",
    "* Time Rewards: -1\n",
    "* pond Rewards: 0\n",
    "* croissant Rewards: 0\n",
    "* cogs Rewards: 0\n",
    "* word Rewards: 20\n",
    "\n",
    "Alpha = 0.1\n",
    "Gamma = 0.8\n",
    "\n",
    "We have to change the hyperparams to make the agent learn and avoid loops.\n",
    "If we work with gamma less than 0.8 the agent won't learn and he will stuck in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1133d1c9-5cb2-4818-aeab-1ce0a76dc413",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning = Q_Learning()\n",
    "q_learning.max_episodes = 1000\n",
    "q_learning.rewards={'r_time':-1,'r_pond':0,'r_croissant':0,'r_cogs':0,'r_work':20}\n",
    "q_learning.tubes=[[(0, 0), (1, 5)], [(1, 2), (5, 2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f064d76-3639-493d-9e51-f3b69e619c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.97 s, sys: 15 ms, total: 4.99 s\n",
      "Wall time: 4.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.8,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0f7b032-15c8-43ed-b7d3-2e57599a72ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  [6, 0, 11, 10, 16, 22, 23, 29, 35]\n",
      "Total Rewards:  15.0\n",
      "Number of steps:  8\n"
     ]
    }
   ],
   "source": [
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b21ba1b-edae-4a56-81d3-537f98f928a1",
   "metadata": {},
   "source": [
    "Here the agent learned to use the tube to acheive his destination with minimal steps, because each step substracts -1 from the total rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a389323-f48c-46b9-b303-45b6b2d71b74",
   "metadata": {},
   "source": [
    "## Experiment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee61aa-02fd-4db8-983e-1c19e25cacce",
   "metadata": {},
   "source": [
    "In this experiment:\n",
    "* Time Rewards: -5\n",
    "* pond Rewards: 0\n",
    "* croissant Rewards: 0\n",
    "* cogs Rewards: 0\n",
    "* word Rewards: 20\n",
    "\n",
    "Alpha = 0.1\n",
    "Gamma = 0.8\n",
    "\n",
    "Let's try if the agent can learn to stay in his position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "515adf47-3612-46c1-b2dc-5e759a2298e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning = Q_Learning()\n",
    "q_learning.max_episodes = 100\n",
    "q_learning.rewards={'r_time':-5,'r_pond':0,'r_croissant':0,'r_cogs':0,'r_work':20}\n",
    "q_learning.tubes=[[(0, 0), (1, 5)], [(1, 2), (4, 1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e187adc-d84f-4938-bd62-e6cf90d0cf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 4.85 ms, total: 1.02 s\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.8,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e567523-2a95-4711-b222-b2cfa78d03d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Agent chose to stay in his position\n",
      "Path:  [6]\n",
      "Total Rewards:  0\n",
      "Number of steps:  0\n"
     ]
    }
   ],
   "source": [
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650092b2-0a34-4462-ab2c-53acf2f5a40b",
   "metadata": {},
   "source": [
    "## Experiment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b8987-77be-499f-b575-fc0bf548dc4e",
   "metadata": {},
   "source": [
    "In this experiment:\n",
    "* Time Rewards: -0.5\n",
    "* pond Rewards: -5\n",
    "* croissant Rewards: 0\n",
    "* cogs Rewards: 0\n",
    "* word Rewards: 20\n",
    "\n",
    "Alpha = 0.1\n",
    "Gamma = 0.9\n",
    "\n",
    "In Experiment 2, the agent found the best path to take to acheive his destination.\n",
    "\n",
    "* Path = [6, 0, 11, 10, 16, 22, 23, 29, 35]\n",
    "* Rewards= 14.0\n",
    "\n",
    "Now we will change the tube to be from [0,0] to [0,5]\n",
    "\n",
    "On his way, his passed throught 16. Now we assigned -5 if the agent falls into a pond.\n",
    "Let's see what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdb6dc95-9db2-4205-84af-758ada721d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning = Q_Learning()\n",
    "q_learning.max_episodes = 3000\n",
    "q_learning.rewards={'r_time':-0.5,'r_pond':-5,'r_croissant':0,'r_cogs':0,'r_work':20}\n",
    "q_learning.tubes=[[(0, 0), (1, 5)], [(1, 2), (5, 2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed4c100d-a296-4eda-991b-98f3c32b6d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 s, sys: 43.4 ms, total: 16.8 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.9,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5785d781-6c2b-4eb2-9bc6-b8a874c2c5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  [6, 7, 8, 14, 20, 21, 22, 23, 29, 35]\n",
      "Total Rewards:  16.0\n",
      "Number of steps:  9\n"
     ]
    }
   ],
   "source": [
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c790b417-e41a-47ad-b219-14b4368e2623",
   "metadata": {},
   "source": [
    "The agent chose not to use the tube! \n",
    "How clever you are my agent!\n",
    "\n",
    "The path using the tube will takes 10 steps, but the agent found a path with 9 steps only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741802cc-af17-43fc-a524-3cd306063c52",
   "metadata": {},
   "source": [
    "## Experiment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80446c15-8fd7-4f61-916e-e46e1ddb9d57",
   "metadata": {},
   "source": [
    "In this experiment:\n",
    "* Time Rewards: -0.5\n",
    "* pond Rewards: -5\n",
    "* croissant Rewards: 0\n",
    "* cogs Rewards: 0\n",
    "* word Rewards: 10\n",
    "\n",
    "Alpha = 0.1\n",
    "Gamma = 0.9\n",
    "\n",
    "Now we will change the blue tube to be from [1,2] to [5,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae9a8ce-a1c7-4058-afb7-e55024a8ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning = Q_Learning()\n",
    "q_learning.max_episodes = 1000\n",
    "q_learning.rewards={'r_time':-0.5,'r_pond':-5,'r_croissant':0,'r_cogs':0,'r_work':10}\n",
    "q_learning.tubes=[[(0, 0), (1, 5)], [(1, 2), (5, 3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11208aff-fcc9-4018-9a93-6359c09a9094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.27 s, sys: 10.4 ms, total: 4.28 s\n",
      "Wall time: 4.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.9,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f0befa6-d291-429a-ba67-54163472f66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  [6, 7, 8, 33, 34, 35]\n",
      "Total Rewards:  8.0\n",
      "Number of steps:  5\n"
     ]
    }
   ],
   "source": [
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da24bb8-25dc-47f9-b853-32e919668c64",
   "metadata": {},
   "source": [
    "The agend learned how to use the bleu tube now, and acheive his destination with only 5 moves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b211e-a3df-4441-aa97-3a7e67124610",
   "metadata": {},
   "source": [
    "## Experiment 6: Limitation of Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec612f-068c-471d-9375-3f71d1d1d351",
   "metadata": {},
   "source": [
    "what if we have positive rewards other than going to work. what the agent reaction will be ?\n",
    "\n",
    "In this experiment:\n",
    "* Time Rewards: 0\n",
    "* pond Rewards: -1\n",
    "* croissant Rewards: 5\n",
    "* cogs Rewards: 10\n",
    "* word Rewards: 20\n",
    "\n",
    "Alpha = 0.1\n",
    "Gamma = 0.9\n",
    "Epsilon = 0.2\n",
    "\n",
    "Now we will change the red tube to be from [0,0] to [3,4], and the blue tube to be from [1,2] to [5,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af05ac46-d446-4a2f-bbb4-6e889eff0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning = Q_Learning()\n",
    "q_learning.max_episodes = 1000\n",
    "q_learning.rewards={'r_time':0,'r_pond':-1,'r_croissant':5,'r_cogs':10,'r_work':20}\n",
    "q_learning.tubes=[[(0, 0), (1,5)], [(1, 2), (5, 1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b35f9932-9245-4070-b2c2-628c4b122f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.42 s, sys: 82.9 ms, total: 7.5 s\n",
      "Wall time: 7.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aea79c52-7c58-43a2-80fa-678d61f3f61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  [6, 7, 8, 31, 32, 26, 27, 33, 34, 35]\n",
      "Total Rewards:  29.0\n",
      "Number of steps:  9\n"
     ]
    }
   ],
   "source": [
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583bc39d-346a-4bdf-8882-5536c27feed7",
   "metadata": {},
   "source": [
    "If we try to run it again maybe we won't get the same resutl, because the epsilon is very small.\n",
    "\n",
    "We are using small value of epsilon because we don't want our agent to explore to much and get stucked in a loop between cells 32 and 26. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be6ae184-248f-4460-928b-bfd71d73eed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  [6, 12, 13, 19, 20, 26, 27, 33, 34, 35]\n",
      "Total Rewards:  19.0\n",
      "Number of steps:  9\n",
      "CPU times: user 972 ms, sys: 4.97 ms, total: 977 ms\n",
      "Wall time: 976 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.8,0.2)\n",
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c9cb72d-0dd5-4b58-9e5f-71f5663a67e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_98857/396555593.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'r_time'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r_pond'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r_croissant'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r_cogs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r_work'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtubes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mq_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'pond'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cogs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'croissant'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "q_learning = Q_Learning()\n",
    "q_learning.max_episodes = 1000\n",
    "q_learning.rewards={'r_time':0,'r_pond':-1,'r_croissant':5,'r_cogs':10,'r_work':20}\n",
    "q_learning.tubes=[[(0, 0), (1,5)], [(1, 2), (5, 1)]]\n",
    "q_learning.positions={'pond': [(2, 4), (4, 3)], 'cogs': [(5, 2)], 'croissant': [(5, 3)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8f7e7-a460-437e-b684-f4eac96de1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Q, Rtot = q_learning.learn(0.1,0.8,0.2)\n",
    "path = find_path(Q)\n",
    "r, steps = calculate_rewards(q_learning.R, path)\n",
    "print(\"Path: \",path)\n",
    "print(\"Total Rewards: \", r)\n",
    "print(\"Number of steps: \",steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
